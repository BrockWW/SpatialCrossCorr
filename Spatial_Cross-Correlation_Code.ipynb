{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014f59e-ee99-45a6-a8d1-e544fc1fe58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "\n",
    "# include path to bubble finding algorithm here\n",
    "sys.path.insert(1, '/mnt/home/wallinbr/Python3.6.4/BubbleFindingAlgorithm/.')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bubble_algorithm_class import *\n",
    "from bubble_plotting_functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import yt\n",
    "from unyt import Gyr\n",
    "\n",
    "# libraries used only for this file\n",
    "from numba import jit\n",
    "import treecorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec07d9-cbd1-4b66-ba9e-860c23039c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "# Variables to change based on galaxy images used\n",
    "################################################################################################\n",
    "'''Variables:\n",
    "    - seed: random seed for numpy to keep noise algorithm results the same (optional)\n",
    "    - smooth_ims: boolean to decide whether to apply a smoothing gaussian to images based\n",
    "                  on the observation galaxy's beam size\n",
    "    - smooth: multiple of observation image's beam used to create smoothing gaussian\n",
    "    - pixel_size: angular pixel side length for observation galaxy image (arcsec)\n",
    "    - beam_size: beam semimajor and semiminor axis from observation galaxy image (arcsec)\n",
    "    - obs_gal_dist: physical distance to observation galaxy (kpc)\n",
    "    - sim_diam_kpc: physical size of simulationgalaxy image if used (kpc)\n",
    "    - obs_file: file path of saved observation column density data .npy file (exlude extension)\n",
    "    - obs_file: file path of saved simulation column density data .npy file (exlude extension)\n",
    "    '''\n",
    "\n",
    "seed = 92947490\n",
    "smooth_ims = True\n",
    "smooth = 2\n",
    "pixel_size = 1.5   # arecseconds\n",
    "beam_size = np.array([6.04, 6.04])   # arcseconds\n",
    "obs_gal_dist = 5.9*1000   # kpc\n",
    "sim_diam_kpc = 40   # kpc\n",
    "\n",
    "obs_file = \"NGC6946\"\n",
    "sim_file = \"inclined_atomicH_colden\"\n",
    "\n",
    "################################################################################################\n",
    "# Loading in galaxy images\n",
    "################################################################################################\n",
    "obs_im = np.load(obs_file + \".npy\")   # H*cm^-2\n",
    "sim_im = np.load(sim_file + \".npy\")   # H*cm^-2\n",
    "\n",
    "obs_im = np.flip(obs_im, axis = 0)\n",
    "\n",
    "################################################################################################\n",
    "# Finding physical size of each real galaxy\n",
    "################################################################################################\n",
    "\n",
    "# angular to physical size formula for some real galaxy\n",
    "obs_pixel_size = pixel_size / 206265  # arcsec -> rad\n",
    "obs_diam_kpc = (obs_pixel_size*obs_im.shape[0]) * obs_gal_dist   # kpc\n",
    "\n",
    "################################################################################################\n",
    "# Applying the observation image's beam and noise to the simulation image\n",
    "################################################################################################\n",
    "\n",
    "sim_beam = apply_beam(beam_size, sim_im.shape[0], pixel_size, sim_im)   # sim image with beam\n",
    "sim_noise_beam, _ = sim_noise_alg(sim_beam, obs_im, seed)   # sim image with noise and beam\n",
    "\n",
    "################################################################################################\n",
    "# Using the Apply Beam algorithm to smooth simulation and observation images (optional)\n",
    "################################################################################################\n",
    "\n",
    "if(smooth_ims == True):\n",
    "    # smoothing each galaxy with a gaussian of size a multiple of observation's beam size\n",
    "    smoothing_axes = smooth*beam_size\n",
    "    out_obs_im = apply_beam(smoothing_axes, obs_im.shape[0], pixel_size, obs_im)\n",
    "    out_sim_im = apply_beam(smoothing_axes, sim_noise_beam.shape[0], pixel_size, sim_noise_beam)\n",
    "\n",
    "else:\n",
    "    out_obs_im = obs_im\n",
    "    out_sim_im = sim_noise_beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f9679-bf23-4daa-b8d1-3e1f41782e86",
   "metadata": {},
   "source": [
    "## Star Mass in Bubbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed669c-17ec-4766-9b06-251f7438d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Functions for finding distributions'''\n",
    "\n",
    "def histogram_sample(data_x, data_y, num, max_val):\n",
    "    data_rad = np.sqrt(data_x**2 + data_y**2)\n",
    "    min_val = np.min(data_rad)\n",
    "    rad_arr = np.geomspace(min_val, max_val, num)\n",
    "    n, b, _ = plt.hist(data_rad, bins = rad_arr)\n",
    "    plt.close()\n",
    "    bin_endpoints = rad_arr[1:]\n",
    "    cdf = np.cumsum(n)\n",
    "    cdf /= cdf[-1]\n",
    "\n",
    "    return cdf, bin_endpoints\n",
    "\n",
    "def random_sample(cdf, bins, N):\n",
    "    values = np.random.rand(N)\n",
    "    value_bins = np.searchsorted(cdf, values)\n",
    "    random_from_cdf = bins[value_bins]\n",
    "\n",
    "    return random_from_cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76d9e0-94b2-433e-a943-501c601195d9",
   "metadata": {},
   "source": [
    "### Spatial Corss-Correlation of Star Particles and Superbubbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf1e45-7a10-4299-8624-a7018ba7ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Collecting all pre-processing items together\n",
    "#####################################################\n",
    "\n",
    "\n",
    "'''Functions for finding distance between each point and the number of pairs'''\n",
    "\n",
    "@jit\n",
    "def data_pairs(data1, data2):\n",
    "    # creating matrix for all distance pairs\n",
    "    dist_matrix = np.zeros(shape = (len(data1), len(data2)))\n",
    "    \n",
    "    # finding all distances between data points\n",
    "    for i in range(len(data1)):\n",
    "        for j in range(len(data2)):\n",
    "            dist_matrix[i, j] = np.sqrt((data1[i,0]-data2[j,0])**2 + (data1[i,1]-data2[j,1])**2)\n",
    "    \n",
    "    return dist_matrix\n",
    "\n",
    "def find_pairs(dist_matrix, r_0, r):\n",
    "    # finding distances within radius\n",
    "    upper_dist = dist_matrix <= r\n",
    "    lower_dist = dist_matrix > r_0\n",
    "    comb_mask = np.logical_and(upper_dist, lower_dist)\n",
    "    \n",
    "    # counting number of pairs\n",
    "    data_keep = dist_matrix[comb_mask]\n",
    "    pairs = data_keep.shape[0]\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "'''Trying to find spatial cross correlation of star particles and superbubbles'''\n",
    "\n",
    "star_x = np.load(\"inclined_x.npy\")\n",
    "star_y = np.load(\"inclined_y.npy\")\n",
    "\n",
    "\n",
    "'''Finding uninclined star particle locations to construct distribution to then incline'''\n",
    "\n",
    "o_star_coords = np.load(\"original_coords.npy\")\n",
    "\n",
    "\n",
    "'''Collecting star location and bubble centroids'''\n",
    "\n",
    "# running superbubble finding algorithm\n",
    "galaxy_obj = Bubble_Alg(out_sim_im, sim_diam_kpc, 50, beam_size[0], pixel_size)\n",
    "\n",
    "# finding all cell locations where star mass exists\n",
    "star_loc = []\n",
    "for loc in zip(star_x, star_y): \n",
    "    star_loc.append(loc)\n",
    "star_loc = np.array(star_loc)\n",
    "print(\"Num stars:\", len(star_loc))\n",
    "\n",
    "# finding superbubble center locations\n",
    "bub_centroids_x = []\n",
    "bub_centroids_y = []\n",
    "\n",
    "for num in galaxy_obj.bubble_arr:\n",
    "    convert_arr = (np.array(galaxy_obj.info_dict[num-1][\"centroid\"])-512)*(40/1024)\n",
    "    bub_centroids_x.append(convert_arr[1])\n",
    "    bub_centroids_y.append(-1*convert_arr[0])\n",
    "\n",
    "bub_centroids = [i for i in zip(bub_centroids_x, bub_centroids_y)]\n",
    "bub_centroids = np.array(bub_centroids)\n",
    "print(\"Num superbubbles:\", len(bub_centroids))\n",
    "\n",
    "\n",
    "'''Getting set of possible points for random point selection'''\n",
    "\n",
    "galaxy_loc = []\n",
    "not_nan = np.logical_not(np.isnan(galaxy_obj.ext_rem_im))\n",
    "galaxy_where = np.where(not_nan)\n",
    "\n",
    "for loc in zip(galaxy_where[1], galaxy_where[0]): \n",
    "    galaxy_loc.append(loc)\n",
    "    \n",
    "galaxy_loc = (np.array(galaxy_loc)-512)*(40/1024)   # kpc\n",
    "galaxy_loc[:, 1] *= -1\n",
    "\n",
    "\n",
    "'''finding distribution of stars and bubbles'''\n",
    "\n",
    "theta = np.radians(33)\n",
    "uninclined_bub_centroids = np.copy(bub_centroids)\n",
    "uninclined_bub_centroids[:,1] /= np.cos(theta)\n",
    "\n",
    "# finding distribution of uninclined star particles and superbubbles\n",
    "num_bins = 1000\n",
    "bub_cdf, bub_bins = histogram_sample(uninclined_bub_centroids[:,0], uninclined_bub_centroids[:,1], num_bins, 20)\n",
    "star_cdf, star_bins = histogram_sample(o_star_coords[:,0], o_star_coords[:,1], num_bins, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945fd9f-4a63-4151-b2e6-2f5fb73d1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Masking data points into quadrants'''\n",
    "\n",
    "def create_masks(arr):\n",
    "    pos_x_mask = np.ma.getmaskarray(np.ma.masked_greater_equal(arr[:, 0], 0))\n",
    "    pos_y_mask = np.ma.getmaskarray(np.ma.masked_greater_equal(arr[:, 1], 0))\n",
    "    neg_x_mask = np.ma.getmaskarray(np.ma.masked_less(arr[:, 0], 0))\n",
    "    neg_y_mask = np.ma.getmaskarray(np.ma.masked_less(arr[:, 1], 0))\n",
    "\n",
    "    pp_mask = np.logical_and(pos_x_mask, pos_y_mask)\n",
    "    pn_mask = np.logical_and(pos_x_mask, neg_y_mask)\n",
    "    np_mask = np.logical_and(neg_x_mask, pos_y_mask)\n",
    "    nn_mask = np.logical_and(neg_x_mask, neg_y_mask)\n",
    "    \n",
    "    return [pp_mask, np_mask, nn_mask, pn_mask]\n",
    "    \n",
    "\n",
    "# stars\n",
    "m = create_masks(star_loc)\n",
    "\n",
    "Q1_stars = star_loc[m[0]]\n",
    "Q2_stars = star_loc[m[1]]\n",
    "Q3_stars = star_loc[m[2]]\n",
    "Q4_stars = star_loc[m[3]]\n",
    "\n",
    "del(star_loc)\n",
    "\n",
    "\n",
    "# galaxy locations\n",
    "m = create_masks(galaxy_loc)\n",
    "\n",
    "Q1_galaxy = galaxy_loc[m[0]]\n",
    "Q2_galaxy = galaxy_loc[m[1]]\n",
    "Q3_galaxy = galaxy_loc[m[2]]\n",
    "Q4_galaxy = galaxy_loc[m[3]]\n",
    "\n",
    "del(galaxy_loc)\n",
    "\n",
    "\n",
    "# superbubble locations\n",
    "m = create_masks(bub_centroids)\n",
    "\n",
    "Q1_bubs = bub_centroids[m[0]]\n",
    "Q2_bubs = bub_centroids[m[1]]\n",
    "Q3_bubs = bub_centroids[m[2]]\n",
    "Q4_bubs = bub_centroids[m[3]]\n",
    "\n",
    "del(bub_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80c201-df3b-4872-b005-269666a28ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Running rest of correlation code by quadrant'''\n",
    "\n",
    "xi_dict = {}\n",
    "xi_tc_dict = {}\n",
    "var_tc_dict = {}\n",
    "pair_dict = {}\n",
    "\n",
    "galaxy = [Q1_galaxy, Q2_galaxy, Q3_galaxy, Q4_galaxy]\n",
    "stars = [Q1_stars, Q2_stars, Q3_stars, Q4_stars]\n",
    "bubs = [Q1_bubs, Q2_bubs, Q3_bubs, Q4_bubs]\n",
    "\n",
    "D1_patches = {}\n",
    "D2_patches = {}\n",
    "R1_patches = {}\n",
    "R2_patches = {}\n",
    "\n",
    "count = 0\n",
    "for j in range(len(galaxy)):\n",
    "    # creating galaxy out of 3 quadrants only\n",
    "    quad_gal = galaxy.pop(j)\n",
    "    quad_stars = stars.pop(j)\n",
    "    quad_bubs = bubs.pop(j)\n",
    "    \n",
    "    use_gal = np.concatenate(galaxy, axis = 0)\n",
    "    use_stars = np.concatenate(stars, axis = 0)\n",
    "    use_bubs = np.concatenate(bubs, axis = 0)\n",
    "\n",
    "    \n",
    "    '''New random point code following distribution'''\n",
    "\n",
    "    # number of random points to use\n",
    "    N1 = 10*len(use_stars)\n",
    "    N2 = 10*len(use_bubs)\n",
    "\n",
    "    # finding random angles to use\n",
    "    if(j == 0):\n",
    "        R1_ang = np.random.uniform(np.pi/2, 2*np.pi, N1)\n",
    "        R2_ang = np.random.uniform(np.pi/2, 2*np.pi, N2)\n",
    "    elif(j == 1):\n",
    "        R1_ang = np.random.uniform(np.pi, 5*np.pi/2, N1)\n",
    "        R2_ang = np.random.uniform(np.pi, 5*np.pi/2, N2)\n",
    "    elif(j == 2):\n",
    "        R1_ang = np.random.uniform(3*np.pi/2, 3*np.pi, N1)\n",
    "        R2_ang = np.random.uniform(3*np.pi/2, 3*np.pi, N2)\n",
    "    elif(j == 3):\n",
    "        R1_ang = np.random.uniform(0, 3*np.pi/2, N1)\n",
    "        R2_ang = np.random.uniform(0, 3*np.pi/2, N2)\n",
    "\n",
    "    # randomly sampling star and bubble radii from actual distribution\n",
    "    R1_rad = random_sample(star_cdf, star_bins, N1)\n",
    "    R2_rad = random_sample(bub_cdf, bub_bins, N2)\n",
    "\n",
    "    # initializing cartesian array\n",
    "    R1 = np.zeros(shape = (N1, 2))\n",
    "    R2 = np.zeros(shape = (N2, 2))\n",
    "\n",
    "    # angle for inclining y-coordinate\n",
    "    inc_angle = np.radians(33)\n",
    "\n",
    "    # converting polar to cartesian - inclining to match galaxy\n",
    "    R1[:, 0] = R1_rad*np.cos(R1_ang)\n",
    "    R1[:, 1] = R1_rad*np.sin(R1_ang)*np.cos(inc_angle)\n",
    "    R2[:, 0] = R2_rad*np.cos(R2_ang)\n",
    "    R2[:, 1] = R2_rad*np.sin(R2_ang)*np.cos(inc_angle)\n",
    "    \n",
    "    # saving patches for TreeCorr code below\n",
    "    D1_patches[j] = use_stars\n",
    "    D2_patches[j] = use_bubs\n",
    "    R1_patches[j] = R1\n",
    "    R2_patches[j] = R2\n",
    "\n",
    "    # finding the distance between sets of points\n",
    "    print(\"D1D2\")\n",
    "    D1D2_dist = data_pairs(use_stars, use_bubs)\n",
    "    print(\"\\nD1R2\")\n",
    "    D1R2_dist = data_pairs(use_stars, R2)\n",
    "    print(\"\\nR1D2\")\n",
    "    R1D2_dist = data_pairs(R1, use_bubs)\n",
    "    print(\"\\nR1R2\")\n",
    "    R1R2_dist = data_pairs(R1, R2)\n",
    "    print(\"\\nDistance Calculation Complete\\n\")\n",
    "    \n",
    "    \n",
    "    # checking minimum distances\n",
    "    print(\"star bubble min distance (kpc):\", np.min(D1D2_dist))\n",
    "    print(\"star random 2 min distance (kpc):\", np.min(D1R2_dist))\n",
    "    print(\"random 1 bubble min distance (kpc):\", np.min(R1D2_dist))\n",
    "    print(\"random 1 random 2 min distance (kpc):\", np.min(R1R2_dist), \"\\n\")\n",
    "    \n",
    "    \n",
    "    '''Finding spatial cross-correlation'''\n",
    "\n",
    "    # radial distances to mask over\n",
    "    r_use = np.geomspace(0.1, 5, 20, endpoint = False)\n",
    "\n",
    "    # normalization factors\n",
    "    N1_norm = len(use_stars)/len(R1)\n",
    "    N2_norm = len(use_bubs)/len(R2)\n",
    "\n",
    "    print(\"Norm1\", N1_norm)\n",
    "    print(\"Norm2\", N2_norm)\n",
    "\n",
    "    pair_dict[\"D1D2\"] = []\n",
    "    pair_dict[\"D1R2\"] = []\n",
    "    pair_dict[\"R1D2\"] = []\n",
    "    pair_dict[\"R1R2\"] = []\n",
    "\n",
    "    # calculating spatial cross-correlation given each radius\n",
    "    xi_arr = np.zeros(len(r_use)-1, dtype = np.float64)\n",
    "    for i in range(len(r_use)-1):\n",
    "        r_0 = r_use[i]\n",
    "        r = r_use[i+1]\n",
    "\n",
    "        # finding total number of pairs\n",
    "        D1D2 = find_pairs(D1D2_dist, r_0, r)\n",
    "        D1R2 = find_pairs(D1R2_dist, r_0, r)\n",
    "        R1D2 = find_pairs(R1D2_dist, r_0, r)\n",
    "        R1R2 = find_pairs(R1R2_dist, r_0, r)\n",
    "\n",
    "        pair_dict[\"D1D2\"].append(D1D2)\n",
    "        pair_dict[\"D1R2\"].append(D1R2)\n",
    "        pair_dict[\"R1D2\"].append(R1D2)\n",
    "        pair_dict[\"R1R2\"].append(R1R2)\n",
    "\n",
    "        # calculating spatial cross-correlation coefficient\n",
    "        try:\n",
    "            xi = (D1D2 - D1R2*N2_norm - R1D2*N1_norm + R1R2*N1_norm*N2_norm)/(R1R2*N1_norm*N2_norm)\n",
    "            xi_arr[i] = xi\n",
    "        except:\n",
    "            xi_arr[i] = np.nan\n",
    "            pass\n",
    "\n",
    "    xi_dict[count] = xi_arr\n",
    "    count += 1\n",
    "    \n",
    "    galaxy.insert(j, quad_gal)\n",
    "    stars.insert(j, quad_stars)\n",
    "    bubs.insert(j, quad_bubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dafdef-b171-4b4d-a0bb-be750e5c42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TreeCorr Correlation Code'''\n",
    "\n",
    "# initializing count-count correlation solver\n",
    "nn = treecorr.NNCorrelation(nbins = 20, min_sep = 1e-1, max_sep = 5, var_method='jackknife')\n",
    "rr = treecorr.NNCorrelation(nbins = 20, min_sep = 1e-1, max_sep = 5, var_method='jackknife')\n",
    "dr = treecorr.NNCorrelation(nbins = 20, min_sep = 1e-1, max_sep = 5, var_method='jackknife')\n",
    "rd = treecorr.NNCorrelation(nbins = 20, min_sep = 1e-1, max_sep = 5, var_method='jackknife')\n",
    "\n",
    "# empty lists to append\n",
    "D1_all = []\n",
    "D2_all = []\n",
    "R1_all = []\n",
    "R2_all = []\n",
    "\n",
    "D1_patch_num = []\n",
    "D2_patch_num = []\n",
    "R1_patch_num = []\n",
    "R2_patch_num = []\n",
    "\n",
    "# running through each patch\n",
    "for i in D1_patches.keys():\n",
    "    D1_all.append(D1_patches[i])\n",
    "    D2_all.append(D2_patches[i])\n",
    "    R1_all.append(R1_patches[i])\n",
    "    R2_all.append(R2_patches[i])\n",
    "\n",
    "    D1_patch_num.append(i*np.ones(len(D1_patches[i]), dtype = np.int64))\n",
    "    D2_patch_num.append(i*np.ones(len(D2_patches[i]), dtype = np.int64))\n",
    "    R1_patch_num.append(i*np.ones(len(R1_patches[i]), dtype = np.int64))\n",
    "    R2_patch_num.append(i*np.ones(len(R2_patches[i]), dtype = np.int64))\n",
    "\n",
    "# combining all data\n",
    "D1_all = np.concatenate(D1_all)\n",
    "D2_all = np.concatenate(D2_all)\n",
    "R1_all = np.concatenate(R1_all)\n",
    "R2_all = np.concatenate(R2_all)\n",
    "\n",
    "# assigning patch number to each element in all data array\n",
    "D1_patch_num = np.concatenate(D1_patch_num)\n",
    "D2_patch_num = np.concatenate(D2_patch_num)\n",
    "R1_patch_num = np.concatenate(R1_patch_num)\n",
    "R2_patch_num = np.concatenate(R2_patch_num)\n",
    "\n",
    "# creating catalog using patches\n",
    "star_cat = treecorr.Catalog(x=D1_all[:,0], y=D1_all[:,1], patch = D1_patch_num)\n",
    "bub_cat = treecorr.Catalog(x=D2_all[:,0], y=D2_all[:,1], patch = D2_patch_num)\n",
    "R1_cat = treecorr.Catalog(x=R1_all[:,0], y=R1_all[:,1], patch = R1_patch_num)\n",
    "R2_cat = treecorr.Catalog(x=R2_all[:,0], y=R2_all[:,1], patch = R2_patch_num)\n",
    "\n",
    "# processing cross correlation\n",
    "nn.process(star_cat, bub_cat)\n",
    "rr.process(R1_cat, R2_cat)\n",
    "dr.process(star_cat, R2_cat)\n",
    "rd.process(R1_cat, bub_cat)\n",
    "\n",
    "# calculating cross correlation values\n",
    "xi_tc, varxi_tc = nn.calculateXi(rr = rr, dr = dr, rd = rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655cf5b-1709-4ea3-aed6-c65ecaeb6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Using jacknife estimator for statistical analysis and error bars'''\n",
    "\n",
    "# getting each jackknife combination correlation values\n",
    "n = len(xi_dict.keys())\n",
    "\n",
    "# finding jackknife mean\n",
    "jackknife_mean = np.zeros_like(xi_dict[0], dtype = np.float64)\n",
    "for j in xi_dict.keys():\n",
    "    jackknife_mean += xi_dict[j]\n",
    "\n",
    "jackknife_mean /= n\n",
    "\n",
    "# finding jackknife standard error\n",
    "jackknife_var = np.zeros(shape = (len(xi_dict[0]), len(xi_dict[0])), dtype = np.float64)\n",
    "for k in xi_dict.keys():\n",
    "    v = xi_dict[k] - jackknife_mean\n",
    "    v_shape = len(v)\n",
    "    v = v.reshape(1, v_shape)\n",
    "    v_t = v.reshape(v_shape, 1)\n",
    "    jackknife_var += np.matmul(v_t, v)\n",
    "    \n",
    "jackknife_var *= (n-1)/n\n",
    "\n",
    "jackknife_error_bars = np.zeros_like(xi_dict[0], dtype = np.float64)\n",
    "for i in range(len(xi_dict[0])):\n",
    "    jackknife_error_bars[i] = np.sqrt(jackknife_var[i,i])\n",
    "\n",
    "log_r = np.log(r_use)\n",
    "mid = (np.log(r_use[1])-np.log(r_use[0]))/2\n",
    "mid_bins = np.exp(log_r+mid)\n",
    "mid_bins = mid_bins[:-1]\n",
    "\n",
    "ew = 0.2\n",
    "plt.errorbar(mid_bins, jackknife_mean, yerr = jackknife_error_bars, color = \"blue\", \n",
    "             capsize = 0.5, elinewidth = ew, label = \" Our Code\")\n",
    "\n",
    "plt.errorbar(np.exp(nn.logr), xi_tc, yerr = varxi_tc, color = \"Red\", \n",
    "             capsize = 0.5, elinewidth = ew, label = \" TC\")\n",
    "\n",
    "plt.ylabel(\"Spatial Cross-Correlation\")\n",
    "plt.xlabel(r\"Radial Separation $\\left(\\mathrm{kpc}\\right)$\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.ylim([-1,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949fb3e-86d8-4b38-a592-75de9bb06159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
